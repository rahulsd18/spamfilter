{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import pytz\n",
    "import graphviz\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from random import randint\n",
    "\n",
    "## Elastic Search for Metrics\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB         \n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# KNN Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Gradient Booster Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading file and looking into the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",sep='\\t',names=['label','text'])\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     label\n",
       "label          \n",
       "ham    0.865937\n",
       "spam   0.134063"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "pd.crosstab(raw_data['label'],columns = 'label',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Train Fit\n",
    "\n",
    "# Define X and y.\n",
    "X = raw_data.text\n",
    "y = raw_data.label\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Ham: 0.8624401913875598\n",
      "Percent Spam: 0.13755980861244022\n"
     ]
    }
   ],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test=='ham', 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent Ham:', y_test_binary.mean())\n",
    "print('Percent Spam:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to cleanup the data through pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics and Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score,test_scores_csv_means_std, y_test,y_pred):\n",
    "\n",
    "    macro_score = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    micro_score = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    weighted_score = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "  \n",
    "\n",
    "    macro_score_insert = {'macro_precision': macro_score[0] * 100, 'macro_recall': macro_score[1]  * 100, 'macro_fscore':macro_score[2]  * 100}\n",
    "    micro_score_insert = {'micro_precision': micro_score[0] * 100, 'micro_recall': micro_score[1] * 100, 'micro_fscore':micro_score[2] * 100}\n",
    "    weighted_score_insert = {'weighted_precision': weighted_score[0] * 100, 'weighted_recall': weighted_score[1] * 100, 'weighted_fscore':weighted_score[2] * 100}\n",
    "    score_insert = {'score': score  * 100}\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Print Accuracy of the current Test\n",
    "    print(algorithm_name_insert , ' pipeline test accuracy: %.3f' % score)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ## Push the data to ElasticSearch\n",
    "\n",
    "    ES_Metric_Insert(run_id_insert, algorithm_name_insert, test_parameters_insert,gs_best_parameters_pipe_spam_ham, score_insert,test_scores_csv_means_std, macro_score_insert,micro_score_insert,weighted_score_insert)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Data into Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_Metric_Insert(run_id_insert,algorithm_name, test_parameters, gs_best_parameters_pipe_spam_ham, score, test_scores_csv_means_std, macro_scores, micro_scores, weighted_scores):\n",
    "    es = Elasticsearch()\n",
    "    \n",
    "    final_dict = {}\n",
    "    \n",
    "    my_current_time = datetime.now(tz=pytz.utc)\n",
    "    timestamp_insert = {'timestamp': my_current_time}\n",
    "    author_insert = {'author': 'Rahul'}\n",
    "    final_dict.update(run_id_insert)\n",
    "    final_dict.update(timestamp_insert)\n",
    "    final_dict.update(author_insert)\n",
    "    final_dict.update(algorithm_name)\n",
    "    final_dict.update(test_parameters)\n",
    "    final_dict.update(gs_best_parameters_pipe_spam_ham)\n",
    "    final_dict.update(score)\n",
    "    final_dict.update(test_scores_csv_means_std)\n",
    "    final_dict.update(macro_scores)\n",
    "    final_dict.update(micro_scores)\n",
    "    final_dict.update(weighted_scores)\n",
    "        \n",
    "    res = es.index(index=\"ml-performance-metrics\", doc_type='text', body=final_dict)\n",
    "    es.indices.refresh(index=\"ml-performance-metrics\")\n",
    "\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name):\n",
    "    \n",
    "    gs_clf_pipe_spam_ham.fit(X_train, y_train)\n",
    "\n",
    "    ## Find predictions for the pipeline\n",
    "    y_pred = gs_clf_pipe_spam_ham.predict(X_test)\n",
    "    \n",
    "    ## Find score of predictions\n",
    "    score_pipe_spam_ham = gs_clf_pipe_spam_ham.score(X_test, y_test)    \n",
    "    \n",
    "    ## Best Grid Search Parameters selected for this case    \n",
    "    gs_best_parameters_pipe_spam_ham = {}\n",
    "    for param_name in sorted(grid_search_parameters.keys()):\n",
    "        gs_best_parameters_pipe_spam_ham[param_name] = gs_clf_pipe_spam_ham.best_params_[param_name]\n",
    "        \n",
    "    \n",
    "    ## Setting up for reporting to Screen and ElasticSearch\n",
    "    \n",
    "    ## Add Run Id for each run. This helps with fishing out the correct dataset in cloud\n",
    "    run_id_insert = {'run_id' : run_id}\n",
    "    \n",
    "    ## Save Classifier name as a string\n",
    "    \n",
    "    classifier_string = str(classifier_name)\n",
    "    classifer_name_only = classifier_string.split(\"(\")[0]\n",
    "    \n",
    "    algorithm_name_insert = {'Algorithm_Name' : classifer_name_only}\n",
    "    \n",
    "    ## Add Classifier Parameters to output\n",
    "    test_parameters_insert = {'test_parameters' : str(pipe_spam_ham)}\n",
    "    \n",
    "    \n",
    "    ## Breaking test cv scores and calculating mean and standard Deviation of each.\n",
    "    cv_scores_df = pd.DataFrame.from_dict(gs_clf_pipe_spam_ham.cv_results_)\n",
    "    \n",
    "    test_scores_csv_means_std = {}\n",
    "    \n",
    "    test_scores_csv_means_std['mean_fit_time'] = cv_scores_df.loc[0 ,'mean_fit_time']\n",
    "    test_scores_csv_means_std['std_fit_time'] = cv_scores_df.loc[0 ,'std_fit_time']\n",
    "    test_scores_csv_means_std['mean_test_score'] = cv_scores_df.loc[0 ,'mean_test_score'] * 100\n",
    "    test_scores_csv_means_std['std_test_score'] = cv_scores_df.loc[0 ,'std_test_score']\n",
    "    test_scores_csv_means_std['mean_train_score'] = cv_scores_df.loc[0 ,'mean_train_score']  * 100\n",
    "    test_scores_csv_means_std['std_train_score'] = cv_scores_df.loc[0 ,'std_train_score']\n",
    "    \n",
    "\n",
    "    ## Send all the collected data to the metric collection and ES insert system.\n",
    "    calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score_pipe_spam_ham, test_scores_csv_means_std, y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "pipe_spam_ham = []\n",
    "pipe_spam_ham_features = []\n",
    "\n",
    "run_id = randint(100000, 999999)\n",
    "\n",
    "## Cross_Val value\n",
    "cv_value = 2\n",
    "\n",
    "# Define 10 fold cross-validation\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "pipe_spam_ham_features.append(('vect', CountVectorizer()))\n",
    "pipe_spam_ham_features.append(('tfidf', TfidfTransformer()))\n",
    "#pipe_spam_ham_features.append(('hash', HashingVectorizer()))\n",
    "\n",
    "## Initializing the classifier to Naieve Bayes\n",
    "pipe_spam_ham_features.append(('clf', MultinomialNB()))\n",
    "\n",
    "## Putting together the various classification algorithms to use\n",
    "clfs = []\n",
    "clfs.append(MultinomialNB())\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=5))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "clfs.append(LogisticRegression())\n",
    "\n",
    "\n",
    "## Setting up the pipeline\n",
    "pipe_spam_ham = Pipeline(pipe_spam_ham_features)\n",
    "\n",
    "\n",
    "grid_search_parameters = {'vect__stop_words': ('english',None),                          \n",
    "                          'vect__ngram_range': [(1, 1),(1, 2),(1, 3), (1, 4)],\n",
    "                          'vect__max_df': (0.9,1),\n",
    "                          'vect__lowercase': (True, False),\n",
    "                          'vect__binary': (True, False),\n",
    "#                           'vect__tokenizer: (LemmaTokenizer()),\n",
    "                          'tfidf__use_idf': (True, False),\n",
    "                          'tfidf__norm': ('l1','l2','max'),\n",
    "                          'tfidf__smooth_idf': (True, False),\n",
    "                          'tfidf__sublinear_tf': (True, False)\n",
    "                         }\n",
    "\n",
    "print(pipe_spam_ham_features)\n",
    "## Trying out the various possible classifiers:\n",
    "\n",
    "for classifier in clfs:\n",
    "    \n",
    "    ## Adding the classifier to be used\n",
    "    pipe_spam_ham.set_params(clf = classifier)\n",
    "    \n",
    "    gs_clf_pipe_spam_ham = GridSearchCV(pipe_spam_ham, grid_search_parameters, n_jobs=-1, cv=cv_value, return_train_score=True)\n",
    "    \n",
    "    ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test,grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier)\n",
    "    \n",
    "    \n",
    "\n",
    "##                          'vect__analyzer': (‘word’, ‘char’, ‘char_wb’),\n",
    "##                          'vect__preprocessor: ('callable','None'),\n",
    "##                          'vect__tokenizer: (LemmaTokenizer()),\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
