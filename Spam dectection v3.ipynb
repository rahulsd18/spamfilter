{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import pytz\n",
    "import graphviz\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from random import randint\n",
    "\n",
    "## Elastic Search for Metrics\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB         \n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# KNN Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Gradient Booster Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading file and looking into the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",sep='\\t',names=['label','text'])\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     label\n",
       "label          \n",
       "ham    0.865937\n",
       "spam   0.134063"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "pd.crosstab(raw_data['label'],columns = 'label',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape (3900, 7234)\n",
      "['yet', 'yetty', 'yetunde', 'yi', 'yijue', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yor', 'yorge', 'you', 'youdoing', 'youi', 'young', 'younger', 'youphone', 'your', 'youre', 'yourinclusive', 'yourjob', 'yours', 'yourself', 'youuuuu', 'youwanna', 'yowifes', 'yr', 'yrs', 'ystrday', 'yummmm', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'zaher', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'Ã¨n']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "print('X_train Shape', X_train_dtm.shape)\n",
    "\n",
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks like we have 7234 Vectors after Count Vectorizer. From 3900 lines of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1179)\t1\n",
      "  (0, 3379)\t1\n",
      "  (0, 3605)\t1\n",
      "  (0, 4374)\t1\n",
      "  (0, 4481)\t1\n",
      "  (0, 5611)\t1\n",
      "  (0, 6317)\t1\n",
      "  (0, 6477)\t1\n",
      "  (0, 7196)\t2\n",
      "  (1, 4598)\t1\n",
      "  (1, 6359)\t1\n",
      "  (2, 1172)\t1\n",
      "  (2, 2031)\t1\n",
      "  (2, 5225)\t1\n",
      "  (2, 6382)\t1\n",
      "  (2, 6985)\t1\n",
      "  (2, 6988)\t1\n",
      "  (2, 7017)\t1\n",
      "  (2, 7196)\t1\n",
      "  (3, 1486)\t1\n",
      "  (3, 3749)\t1\n",
      "  (3, 3872)\t1\n",
      "  (3, 5913)\t1\n",
      "  (4, 677)\t1\n",
      "  (4, 1021)\t1\n",
      "  :\t:\n",
      "  (1669, 3476)\t1\n",
      "  (1669, 3487)\t1\n",
      "  (1669, 4588)\t1\n",
      "  (1669, 6254)\t1\n",
      "  (1669, 6302)\t1\n",
      "  (1669, 6364)\t1\n",
      "  (1670, 868)\t1\n",
      "  (1670, 989)\t1\n",
      "  (1670, 1508)\t1\n",
      "  (1670, 4109)\t1\n",
      "  (1670, 6564)\t1\n",
      "  (1671, 875)\t1\n",
      "  (1671, 1205)\t1\n",
      "  (1671, 1295)\t1\n",
      "  (1671, 1516)\t1\n",
      "  (1671, 2734)\t1\n",
      "  (1671, 2916)\t1\n",
      "  (1671, 3151)\t1\n",
      "  (1671, 3352)\t1\n",
      "  (1671, 3493)\t1\n",
      "  (1671, 4313)\t1\n",
      "  (1671, 4364)\t1\n",
      "  (1671, 5939)\t1\n",
      "  (1671, 7046)\t1\n",
      "  (1671, 7196)\t1\n"
     ]
    }
   ],
   "source": [
    "## Vocabulary used:\n",
    "# vect.vocabulary_\n",
    "\n",
    "print(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Train Fit\n",
    "\n",
    "# Define X and y.\n",
    "X = raw_data.text\n",
    "y = raw_data.label\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Ham: 0.8624401913875598\n",
      "Percent Spam: 0.13755980861244022\n"
     ]
    }
   ],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test=='ham', 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent Ham:', y_test_binary.mean())\n",
    "print('Percent Spam:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to cleanup the data through pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics and Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score,test_scores_csv_means_std, y_test,y_pred):\n",
    "\n",
    "    macro_score = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    micro_score = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    weighted_score = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "  \n",
    "\n",
    "    macro_score_insert = {'macro_precision': macro_score[0] * 100, 'macro_recall': macro_score[1]  * 100, 'macro_fscore':macro_score[2]  * 100}\n",
    "    micro_score_insert = {'micro_precision': micro_score[0] * 100, 'micro_recall': micro_score[1] * 100, 'micro_fscore':micro_score[2] * 100}\n",
    "    weighted_score_insert = {'weighted_precision': weighted_score[0] * 100, 'weighted_recall': weighted_score[1] * 100, 'weighted_fscore':weighted_score[2] * 100}\n",
    "    score_insert = {'score': score}\n",
    "    \n",
    "    print(score_insert)\n",
    "    \n",
    "    ## Print Accuracy of the current Test\n",
    "    print(algorithm_name_insert , ' pipeline test accuracy: %.3f' % score)\n",
    "    \n",
    "    ## Push the data to ElasticSearch\n",
    "\n",
    "    ES_Metric_Insert(run_id_insert, algorithm_name_insert, test_parameters_insert,gs_best_parameters_pipe_spam_ham, score_insert,test_scores_csv_means_std, macro_score_insert,micro_score_insert,weighted_score_insert)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Data into Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_Metric_Insert(run_id_insert,algorithm_name, test_parameters, gs_best_parameters_pipe_spam_ham, score, test_scores_csv_means_std, macro_scores, micro_scores, weighted_scores):\n",
    "    es = Elasticsearch()\n",
    "    \n",
    "    final_dict = {}\n",
    "    \n",
    "    my_current_time = datetime.now(tz=pytz.utc)\n",
    "    timestamp_insert = {'timestamp': my_current_time}\n",
    "    author_insert = {'author': 'Rahul'}\n",
    "    final_dict.update(run_id_insert)\n",
    "    final_dict.update(timestamp_insert)\n",
    "    final_dict.update(author_insert)\n",
    "    final_dict.update(algorithm_name)\n",
    "    final_dict.update(test_parameters)\n",
    "    final_dict.update(gs_best_parameters_pipe_spam_ham)\n",
    "    final_dict.update(score)\n",
    "    final_dict.update(test_scores_csv_means_std)\n",
    "    final_dict.update(macro_scores)\n",
    "    final_dict.update(micro_scores)\n",
    "    final_dict.update(weighted_scores)\n",
    "        \n",
    "    res = es.index(index=\"ml-performance-metrics\", doc_type='text', body=final_dict)\n",
    "    es.indices.refresh(index=\"ml-performance-metrics\")\n",
    "\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the ML Pipeline and Calculate Metrics (using another function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name):\n",
    "    \n",
    "    gs_clf_pipe_spam_ham.fit(X_train, y_train)\n",
    "\n",
    "    ## Find predictions for the pipeline\n",
    "    y_pred = gs_clf_pipe_spam_ham.predict(X_test)\n",
    "    \n",
    "    ## Find score of predictions\n",
    "    score_pipe_spam_ham = gs_clf_pipe_spam_ham.score(X_test, y_test) * 100 \n",
    "    \n",
    "    ## Best Grid Search Parameters selected for this case    \n",
    "    gs_best_parameters_pipe_spam_ham = {}\n",
    "    for param_name in sorted(grid_search_parameters.keys()):\n",
    "        gs_best_parameters_pipe_spam_ham[param_name] = gs_clf_pipe_spam_ham.best_params_[param_name]\n",
    "        \n",
    "    \n",
    "    ## Setting up for reporting to Screen and ElasticSearch\n",
    "    \n",
    "    ## Add Run Id for each run. This helps with fishing out the correct dataset in cloud\n",
    "    run_id_insert = {'run_id' : run_id}\n",
    "    \n",
    "    ## Save Classifier name as a string\n",
    "    \n",
    "    classifier_string = str(classifier_name)\n",
    "    classifer_name_only = classifier_string.split(\"(\")[0]\n",
    "    \n",
    "    algorithm_name_insert = {'Algorithm_Name' : classifer_name_only}\n",
    "    \n",
    "    ## Add Classifier Parameters to output\n",
    "    test_parameters_insert = {'test_parameters' : str(pipe_spam_ham)}\n",
    "    \n",
    "    \n",
    "    ## Breaking test cv scores and calculating mean and standard Deviation of each.\n",
    "    cv_scores_df = pd.DataFrame.from_dict(gs_clf_pipe_spam_ham.cv_results_)\n",
    "    \n",
    "    test_scores_csv_means_std = {}\n",
    "    \n",
    "    test_scores_csv_means_std['mean_fit_time'] = cv_scores_df.loc[0 ,'mean_fit_time']\n",
    "    test_scores_csv_means_std['std_fit_time'] = cv_scores_df.loc[0 ,'std_fit_time']\n",
    "    test_scores_csv_means_std['mean_test_score'] = cv_scores_df.loc[0 ,'mean_test_score'] * 100\n",
    "    test_scores_csv_means_std['std_test_score'] = cv_scores_df.loc[0 ,'std_test_score']\n",
    "    test_scores_csv_means_std['mean_train_score'] = cv_scores_df.loc[0 ,'mean_train_score']  * 100\n",
    "    test_scores_csv_means_std['std_train_score'] = cv_scores_df.loc[0 ,'std_train_score']\n",
    "    \n",
    "\n",
    "    ## Send all the collected data to the metric collection and ES insert system.\n",
    "    calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score_pipe_spam_ham, test_scores_csv_means_std, y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Vectorizers and ML Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vectorizer_ml_algo(vector_ml_keyword):\n",
    "    \n",
    "    ## Remove from gridsearch\n",
    "    for key in grid_search_parameters.copy():\n",
    "         if vector_ml_keyword in key.lower():\n",
    "            del grid_search_parameters[key]\n",
    "    \n",
    "    \n",
    "    ## Remove from spam ham pipeline\n",
    "    \n",
    "    for item in pipe_spam_ham_features:\n",
    "        if vector_ml_keyword in item:\n",
    "            pipe_spam_ham_features.remove(item)\n",
    "    \n",
    "    return()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Count Vectorizer and associated Features for Testing\n",
    "def add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters):\n",
    "\n",
    "    grid_search_parameters['vect__binary'] = (False,True)\n",
    "    grid_search_parameters['vect__lowercase'] = (True,False)\n",
    "    grid_search_parameters['vect__tokenizer'] = (LemmaTokenizer(),None)\n",
    "    \n",
    "##    Grid Search Parameters avialable for testing. After initial tests it looks like the above params work best. So using those. \n",
    "#     grid_search_parameters['vect__stop_words'] = ('english',None)\n",
    "#     grid_search_parameters['vect__ngram_range'] = [(1, 1),(1, 2),(1, 3), (1, 4)]\n",
    "#     grid_search_parameters['vect__max_df'] = (0.9,1)\n",
    "#     grid_search_parameters['vect__lowercase'] = (True, False)\n",
    "#     grid_search_parameters['vect__binary'] = (True, False)\n",
    "#     grid_search_parameters['vect__tokenizer'] = (LemmaTokenizer())\n",
    "#     grid_search_parameters['vect__min_df'] = (5,10)\n",
    "    pipe_spam_ham_features.append(('vect', CountVectorizer()))\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tf-Idf Vectorizer and associated Features for Testing\n",
    "def add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['tfidf__norm'] = ('l2','l1')\n",
    "    grid_search_parameters['tfidf__smooth_idf'] = (True,False)\n",
    "    \n",
    "#     ## Grid Search Parameters avialable for testing. After initial tests it looks like the above params work best. So using those.\n",
    "#     grid_search_parameters['tfidf__use_idf'] = (True, False)\n",
    "#     grid_search_parameters['tfidf__norm'] = ('l1','l2','max')\n",
    "#     grid_search_parameters['tfidf__smooth_idf'] = (True, False)\n",
    "#     grid_search_parameters['tfidf__sublinear_tf'] = (True, False)\n",
    "\n",
    "    pipe_spam_ham_features.append(('tfidf', TfidfVectorizer()))\n",
    "    \n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tf-Idf Vectorizer and associated Features for Testing\n",
    "def add_TruncatedSVD(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['truncatedsvd__n_components'] = (500, 400, 200)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('truncatedsvd', TruncatedSVD()))\n",
    "    \n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Naive Bayes Algorithm\n",
    "def add_multinomialNB(pipe_spam_ham_features,grid_search_parameters):\n",
    "\n",
    "    grid_search_parameters['nb__alpha'] = (1,0.9)\n",
    "    grid_search_parameters['nb__fit_prior'] = (True,False)\n",
    "#     ## Grid Search Parameters avialable for testing. After initial tests it looks like the above params work best. So using those.    \n",
    "#     grid_search_parameters['nb__alpha'] = (0,1)\n",
    "#     grid_search_parameters['nb__fit_prior'] = (True, False)\n",
    "    \n",
    "    \n",
    "    pipe_spam_ham_features.append(('nb', MultinomialNB()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Naive Bayes Algorithm\n",
    "def add_knn(pipe_spam_ham_features,grid_search_parameters):\n",
    "\n",
    "    grid_search_parameters['knn__n_neighbors'] = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15)\n",
    "    grid_search_parameters['knn__weights'] = ('uniform', 'distance')\n",
    "    #grid_search_parameters['knn__algorithm'] = ('ball_tree', 'kd_tree')\n",
    "    \n",
    "    pipe_spam_ham_features.append(('knn', KNeighborsClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Random Forest Algorithm\n",
    "def add_randomforest(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['rf__n_estimators'] = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)\n",
    "    grid_search_parameters['rf__max_depth'] = (10,100,1000,None)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('rf', RandomForestClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Logistic Regression Algorithm\n",
    "def add_logistic_regression(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['lr__penalty'] = ('l1','l2')\n",
    "    \n",
    "    pipe_spam_ham_features.append(('lr', LogisticRegression()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add SVC Algorithm\n",
    "def add_svc_regression(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['svc__C'] = (1.0,0.9,0.8)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('svc', SVC()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add GradientBoostingClassifier Algorithm\n",
    "def add_gradient_boosting_classifer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['gbc__n_estimators'] = (100,200,300,1000)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('gbc', GradientBoostingClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add DecisionTreeClassifier Algorithm\n",
    "def add_decisiontree_classifer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['dtc__max_depth'] = (10,100,1000,None)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('dtc', DecisionTreeClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...owski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f805c0435f8>, None), 'knn__n_neighbors': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), 'knn__weights': ('uniform', 'distance')}\n"
     ]
    }
   ],
   "source": [
    "pipe_spam_ham = []\n",
    "pipe_spam_ham_features = []\n",
    "grid_search_parameters = {}\n",
    "list_ml_algo = {}\n",
    "\n",
    "\n",
    "run_id = randint(100000, 999999)\n",
    "\n",
    "## Cross_Val value\n",
    "cv_value = 2\n",
    "\n",
    "# Define 10 fold cross-validation\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "# ## Addition of Count Vectorizer\n",
    "#add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "\n",
    "## Not using these, since the values score isn't much better than with Count Vectorizer.\n",
    "#add_TruncatedSVD(pipe_spam_ham_features,grid_search_parameters)\n",
    "#add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "\n",
    "\n",
    "\n",
    "## Create a dictionary of all available ML Algos\n",
    "list_ml_algo['knn'] = 'knn'\n",
    "list_ml_algo['rf'] = 'randomforest'\n",
    "list_ml_algo['lr'] = 'logistic_regression'\n",
    "list_ml_algo['nb'] = 'multinomialNB'\n",
    "list_ml_algo['svc'] = 'svc_regression'\n",
    "list_ml_algo['gbc'] = 'gradient_boosting_classifer'\n",
    "list_ml_algo['dtc'] = 'decisiontree_classifer'\n",
    "\n",
    "\n",
    "## Kick off the pipeline Execution:\n",
    "\n",
    "## Iteration 1:\n",
    "## No Vectorizer\n",
    "\n",
    "count = 1\n",
    "\n",
    "while count < 3:\n",
    "    if count == 1:\n",
    "        add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "    if count == 2:\n",
    "        add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "        \n",
    "    for key, values in list_ml_algo.items():\n",
    "        ml_algo_name = 'add_' + values\n",
    "        returnValueIfAny = globals()[ml_algo_name](pipe_spam_ham_features,grid_search_parameters)\n",
    "\n",
    "        ## Setting up the pipeline\n",
    "        pipe_spam_ham = Pipeline(pipe_spam_ham_features)\n",
    "\n",
    "        classifier = str(pipe_spam_ham_features[-1:][0][1])\n",
    "\n",
    "        print(pipe_spam_ham)\n",
    "\n",
    "        print(grid_search_parameters)\n",
    "\n",
    "        ## Adding the GridSearch CV\n",
    "        gs_clf_pipe_spam_ham = GridSearchCV(pipe_spam_ham, grid_search_parameters, n_jobs=1, cv = cv_value, return_train_score=True)\n",
    "\n",
    "        ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test,grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier)\n",
    "\n",
    "        remove_vectorizer_ml_algo(key)\n",
    "\n",
    "    # remove_vectorizer_ml_algo('truncatedsvd')\n",
    "    remove_vectorizer_ml_algo('vect')\n",
    "    remove_vectorizer_ml_algo('tfidf')\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "## End of Program ..        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-25 22:14:04,570 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-07-25 22:14:04,572 : INFO : collecting all words and their counts\n",
      "2018-07-25 22:14:04,573 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2018-07-25 22:14:04,574 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-25 22:14:04,575 : INFO : collected 6 word types from a corpus of 9 raw words and 2 sentences\n",
      "2018-07-25 22:14:04,578 : INFO : Loading a fresh vocabulary\n",
      "2018-07-25 22:14:04,579 : INFO : min_count=2 retains 3 unique words (50% of original 6, drops 3)\n",
      "2018-07-25 22:14:04,580 : INFO : min_count=2 leaves 6 word corpus (66% of original 9, drops 3)\n",
      "2018-07-25 22:14:04,581 : INFO : deleting the raw counts dictionary of 6 items\n",
      "2018-07-25 22:14:04,581 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-07-25 22:14:04,582 : INFO : downsampling leaves estimated 0 word corpus (5.8% of prior 6)\n",
      "2018-07-25 22:14:04,583 : INFO : estimated required memory for 3 words and 150 dimensions: 5100 bytes\n",
      "2018-07-25 22:14:04,584 : INFO : resetting layer weights\n",
      "2018-07-25 22:14:04,585 : INFO : training model with 10 workers on 3 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-25 22:14:04,593 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,594 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,595 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,596 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,596 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,598 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,603 : INFO : EPOCH - 1 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,609 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,610 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,611 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,612 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,615 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,617 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,619 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,622 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,623 : INFO : EPOCH - 2 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,634 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,636 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,638 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,639 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,640 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,641 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,642 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,643 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,644 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,646 : INFO : EPOCH - 3 : training on 9 raw words (1 effective words) took 0.0s, 81 effective words/s\n",
      "2018-07-25 22:14:04,664 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,666 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,670 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,673 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,675 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,677 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,678 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,683 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,684 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,686 : INFO : EPOCH - 4 : training on 9 raw words (1 effective words) took 0.0s, 43 effective words/s\n",
      "2018-07-25 22:14:04,699 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,702 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,704 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,705 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,707 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,708 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,716 : INFO : EPOCH - 5 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,717 : INFO : training on a 45 raw words (2 effective words) took 0.1s, 15 effective words/s\n",
      "2018-07-25 22:14:04,719 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-07-25 22:14:04,720 : INFO : training model with 10 workers on 3 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-25 22:14:04,737 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,739 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,740 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,742 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,743 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,745 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,746 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,752 : INFO : EPOCH - 1 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,753 : WARNING : EPOCH - 1 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,771 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,773 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-25 22:14:04,775 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,776 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,778 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,780 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,781 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,783 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,788 : INFO : EPOCH - 2 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,789 : WARNING : EPOCH - 2 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,805 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,807 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,809 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,810 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,812 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,814 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,815 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,818 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,821 : INFO : EPOCH - 3 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,824 : WARNING : EPOCH - 3 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,837 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,839 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,840 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,842 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,850 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,851 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,853 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,854 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,856 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,857 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,859 : INFO : EPOCH - 4 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,860 : WARNING : EPOCH - 4 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,881 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,884 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,885 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,887 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,889 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,890 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,892 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,893 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,897 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,898 : INFO : EPOCH - 5 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,900 : WARNING : EPOCH - 5 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,918 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,921 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,922 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,924 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,926 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,927 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,929 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,936 : INFO : EPOCH - 6 : training on 9 raw words (1 effective words) took 0.0s, 55 effective words/s\n",
      "2018-07-25 22:14:04,937 : WARNING : EPOCH - 6 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,945 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,947 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,948 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,949 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,952 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,953 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,957 : INFO : EPOCH - 7 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,958 : WARNING : EPOCH - 7 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,970 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,971 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,972 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,973 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,975 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,976 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,978 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,982 : INFO : EPOCH - 8 : training on 9 raw words (1 effective words) took 0.0s, 78 effective words/s\n",
      "2018-07-25 22:14:04,983 : WARNING : EPOCH - 8 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,993 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,995 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,996 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,997 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:05,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-25 22:14:05,004 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:05,006 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:05,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:05,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:05,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:05,013 : INFO : EPOCH - 9 : training on 9 raw words (1 effective words) took 0.0s, 50 effective words/s\n",
      "2018-07-25 22:14:05,014 : WARNING : EPOCH - 9 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:05,030 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:05,033 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:05,035 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:05,036 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:05,038 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:05,039 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:05,041 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:05,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:05,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:05,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:05,047 : INFO : EPOCH - 10 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:05,048 : WARNING : EPOCH - 10 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:05,050 : INFO : training on a 90 raw words (3 effective words) took 0.3s, 9 effective words/s\n",
      "2018-07-25 22:14:05,052 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 90)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports needed and logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "# build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "    raw_data,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)\n",
    "model.train(raw_data, total_examples=len(raw_data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3032         gonna let me know cos comes bak from holiday that day.  is coming. Don't4get2text me  number. \n",
      "3381                                                                      Just finished. Missing you plenty\n",
      "4805    Er, hello, things didnât quite go to plan â is limping slowly home followed by aa and with exhau...\n",
      "504                                                                Lolnice. I went from a fish to ..water.?\n",
      "4459              Die... I accidentally deleted e msg i suppose 2 put in e sim archive. Haiz... I so sad...\n",
      "2125    Beautiful Truth against Gravity.. Read carefully: \"Our heart feels light when someone is in it.....\n",
      "763     Urgent Ur Â£500 guaranteed award is still unclaimed! Call 09066368327 NOW closingdate04/09/02 cla...\n",
      "3423    Freemsg: 1-month unlimited free calls! Activate SmartCall Txt: CALL to No: 68866. Subscriptn3gbp...\n",
      "2284                                                                      I reach home safe n sound liao...\n",
      "2704                                               Yup no more already... Thanx 4 printing n handing it up.\n",
      "967                                         I am not sure about night menu. . . I know only about noon menu\n",
      "169                                     Great escape. I fancy the bridge but needs her lager. See you tomo \n",
      "5070                     Hai dear friends... This is my new &amp; present number..:) By Rajitha Raj (Ranju)\n",
      "267                                                         Not sure yet, still trying to get a hold of him\n",
      "2465                                                      They will pick up and drop in car.so no problem..\n",
      "3229    SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, ...\n",
      "2493    No drama Pls.i have had enough from you and family while i am struggling in the hot sun in a str...\n",
      "1506                                       Total video converter free download type this in google search:)\n",
      "351                                                      Nah can't help you there, I've never had an iphone\n",
      "1757                                                     Lmao ok I wont be needing u to do my hair anymore.\n",
      "464                                                                           Sorry, I'll call later ok bye\n",
      "3345                                                   Oh great. I.ll disturb him more so that we can talk.\n",
      "4464                  He said i look pretty wif long hair wat. But i thk he's cutting quite short 4 me leh.\n",
      "4150    Haven't found a way to get another app for your phone, eh ? Will you go to the net cafe ? Did yo...\n",
      "187                                                                      Haha awesome, be there in a minute\n",
      "93      Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WO...\n",
      "2421                                                   Oic... Then better quickly go bathe n settle down...\n",
      "4447                                                                        Merry christmas to u too annie!\n",
      "2777                                                                           Send me your id and password\n",
      "1439                                                                     Arms fine, how's Cardiff and uni? \n",
      "                                                       ...                                                 \n",
      "1553                                                                                               U too...\n",
      "3134                                                                              So no messages. Had food?\n",
      "4846                                                                      Missing you too.pray inshah allah\n",
      "2878    twenty past five he said will this train have been to durham already or not coz i am in a reserv...\n",
      "1819                                                                           Am i that much dirty fellow?\n",
      "2810                                              Oh yeah I forgot. U can only take 2 out shopping at once.\n",
      "591     For ur chance to win a Â£250 wkly shopping spree TXT: SHOP to 80878. T's&C's www.txt-2-shop.com c...\n",
      "4903    * FREE* POLYPHONIC RINGTONE Text SUPER to 87131 to get your FREE POLY TONE of the week now! 16 S...\n",
      "5179    Hi hope u r both ok, he said he would text and he hasn't, have u seen him, let me down gently pl...\n",
      "1044    Mmm thats better now i got a roast down me! iÂd b better if i had a few drinks down me 2! Good i...\n",
      "4040                                                    I cant pick the phone right now. Pls send a message\n",
      "5544    I'm taking derek &amp; taylor to walmart, if I'm not back by the time you're done just leave the...\n",
      "4748                        When you just put in the + sign, choose my number and the pin will show. Right?\n",
      "5335    No. It's not pride. I'm almost  &lt;#&gt;  years old and shouldn't be takin money from my kid. Y...\n",
      "244     Although i told u dat i'm into baig face watches now but i really like e watch u gave cos it's f...\n",
      "1387                                                                            All e best 4 ur exam later.\n",
      "2022                    I don't have anybody's number, I still haven't thought up a tactful way to ask alex\n",
      "59                                                                 Yes..gauti and sehwag out of odi series.\n",
      "5341                                                                 And of course you should make a stink!\n",
      "176                                       Let me know when you've got the money so carlos can make the call\n",
      "449                                                              LOL ... Have you made plans for new years?\n",
      "4081    Sir, good morning. Hope you had a good weekend. I called to let you know that i was able to rais...\n",
      "55                             Do you know what Mallika Sherawat did yesterday? Find out now @  &lt;URL&gt;\n",
      "3457                                                                           Ok. I.ll do you right later.\n",
      "5188                                                                                                   Okie\n",
      "1768                                                                             K, want us to come by now?\n",
      "1737                                                                                I will come tomorrow di\n",
      "3240                                                               Am okay. Will soon be over. All the best\n",
      "5305        Hi missed your Call and my mumHas beendropping red wine all over theplace! what is your adress?\n",
      "4737                                                       Not for possession, especially not first offense\n",
      "Name: text, Length: 3900, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
