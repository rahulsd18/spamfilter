{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import pytz\n",
    "import graphviz\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from random import randint\n",
    "\n",
    "## Elastic Search for Metrics\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB         \n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# KNN Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Gradient Booster Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading file and looking into the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",sep='\\t',names=['label','text'])\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     label\n",
       "label          \n",
       "ham    0.865937\n",
       "spam   0.134063"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "pd.crosstab(raw_data['label'],columns = 'label',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Train Fit\n",
    "\n",
    "# Define X and y.\n",
    "X = raw_data.text\n",
    "y = raw_data.label\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Ham: 0.8624401913875598\n",
      "Percent Spam: 0.13755980861244022\n"
     ]
    }
   ],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test=='ham', 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent Ham:', y_test_binary.mean())\n",
    "print('Percent Spam:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to cleanup the data through pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics and Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score,test_scores_csv_means_std, y_test,y_pred):\n",
    "\n",
    "    macro_score = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    micro_score = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    weighted_score = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "  \n",
    "\n",
    "    macro_score_insert = {'macro_precision': macro_score[0] * 100, 'macro_recall': macro_score[1]  * 100, 'macro_fscore':macro_score[2]  * 100}\n",
    "    micro_score_insert = {'micro_precision': micro_score[0] * 100, 'micro_recall': micro_score[1] * 100, 'micro_fscore':micro_score[2] * 100}\n",
    "    weighted_score_insert = {'weighted_precision': weighted_score[0] * 100, 'weighted_recall': weighted_score[1] * 100, 'weighted_fscore':weighted_score[2] * 100}\n",
    "    score_insert = {'score': score}\n",
    "    \n",
    "    print(score_insert)\n",
    "    \n",
    "    ## Print Accuracy of the current Test\n",
    "    print(algorithm_name_insert , ' pipeline test accuracy: %.3f' % score)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ## Push the data to ElasticSearch\n",
    "\n",
    "    ES_Metric_Insert(run_id_insert, algorithm_name_insert, test_parameters_insert,gs_best_parameters_pipe_spam_ham, score_insert,test_scores_csv_means_std, macro_score_insert,micro_score_insert,weighted_score_insert)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Data into Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_Metric_Insert(run_id_insert,algorithm_name, test_parameters, gs_best_parameters_pipe_spam_ham, score, test_scores_csv_means_std, macro_scores, micro_scores, weighted_scores):\n",
    "    es = Elasticsearch()\n",
    "    \n",
    "    final_dict = {}\n",
    "    \n",
    "    my_current_time = datetime.now(tz=pytz.utc)\n",
    "    timestamp_insert = {'timestamp': my_current_time}\n",
    "    author_insert = {'author': 'Rahul'}\n",
    "    final_dict.update(run_id_insert)\n",
    "    final_dict.update(timestamp_insert)\n",
    "    final_dict.update(author_insert)\n",
    "    final_dict.update(algorithm_name)\n",
    "    final_dict.update(test_parameters)\n",
    "    final_dict.update(gs_best_parameters_pipe_spam_ham)\n",
    "    final_dict.update(score)\n",
    "    final_dict.update(test_scores_csv_means_std)\n",
    "    final_dict.update(macro_scores)\n",
    "    final_dict.update(micro_scores)\n",
    "    final_dict.update(weighted_scores)\n",
    "        \n",
    "    res = es.index(index=\"ml-performance-metrics\", doc_type='text', body=final_dict)\n",
    "    es.indices.refresh(index=\"ml-performance-metrics\")\n",
    "\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name):\n",
    "    \n",
    "    gs_clf_pipe_spam_ham.fit(X_train, y_train)\n",
    "\n",
    "    ## Find predictions for the pipeline\n",
    "    y_pred = gs_clf_pipe_spam_ham.predict(X_test)\n",
    "    \n",
    "    ## Find score of predictions\n",
    "    score_pipe_spam_ham = gs_clf_pipe_spam_ham.score(X_test, y_test) * 100 \n",
    "    \n",
    "    ## Best Grid Search Parameters selected for this case    \n",
    "    gs_best_parameters_pipe_spam_ham = {}\n",
    "    for param_name in sorted(grid_search_parameters.keys()):\n",
    "        gs_best_parameters_pipe_spam_ham[param_name] = gs_clf_pipe_spam_ham.best_params_[param_name]\n",
    "        \n",
    "    \n",
    "    ## Setting up for reporting to Screen and ElasticSearch\n",
    "    \n",
    "    ## Add Run Id for each run. This helps with fishing out the correct dataset in cloud\n",
    "    run_id_insert = {'run_id' : run_id}\n",
    "    \n",
    "    ## Save Classifier name as a string\n",
    "    \n",
    "    classifier_string = str(classifier_name)\n",
    "    classifer_name_only = classifier_string.split(\"(\")[0]\n",
    "    \n",
    "    algorithm_name_insert = {'Algorithm_Name' : classifer_name_only}\n",
    "    \n",
    "    ## Add Classifier Parameters to output\n",
    "    test_parameters_insert = {'test_parameters' : str(pipe_spam_ham)}\n",
    "    \n",
    "    \n",
    "    ## Breaking test cv scores and calculating mean and standard Deviation of each.\n",
    "    cv_scores_df = pd.DataFrame.from_dict(gs_clf_pipe_spam_ham.cv_results_)\n",
    "    \n",
    "    test_scores_csv_means_std = {}\n",
    "    \n",
    "    test_scores_csv_means_std['mean_fit_time'] = cv_scores_df.loc[0 ,'mean_fit_time']\n",
    "    test_scores_csv_means_std['std_fit_time'] = cv_scores_df.loc[0 ,'std_fit_time']\n",
    "    test_scores_csv_means_std['mean_test_score'] = cv_scores_df.loc[0 ,'mean_test_score'] * 100\n",
    "    test_scores_csv_means_std['std_test_score'] = cv_scores_df.loc[0 ,'std_test_score']\n",
    "    test_scores_csv_means_std['mean_train_score'] = cv_scores_df.loc[0 ,'mean_train_score']  * 100\n",
    "    test_scores_csv_means_std['std_train_score'] = cv_scores_df.loc[0 ,'std_train_score']\n",
    "    \n",
    "\n",
    "    ## Send all the collected data to the metric collection and ES insert system.\n",
    "    calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score_pipe_spam_ham, test_scores_csv_means_std, y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Count Vectorizer and associated Features for Testing\n",
    "def add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    list_count_vect_pipe_and_features = []\n",
    "    \n",
    "    grid_search_parameters['vect__stop_words'] = ('english',None)\n",
    "    grid_search_parameters['vect__ngram_range'] = [(1, 1),(1, 2),(1, 3), (1, 4)]\n",
    "    grid_search_parameters['vect__max_df'] = (0.9,1)\n",
    "    grid_search_parameters['vect__lowercase'] = (True, False)\n",
    "    grid_search_parameters['vect__binary'] = (True, False)\n",
    "#   grid_search_parameters['vect__tokenizer'] = (LemmaTokenizer())\n",
    "\n",
    "    list_count_vect_pipe_and_features.append(('vect', CountVectorizer()))\n",
    "    list_count_vect_pipe_and_features.append(grid_search_parameters)\n",
    "\n",
    "    return(list_count_vect_pipe_and_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tf-Idf Vectorizer and associated Features for Testing\n",
    "def add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    list_tfidf_vect_pipe_and_features = []\n",
    "    \n",
    "    grid_search_parameters['tfidf__use_idf'] = (True, False)\n",
    "    grid_search_parameters['tfidf__norm'] = ('l1','l2','max')\n",
    "    grid_search_parameters['tfidf__use_idf'] = (True, False)\n",
    "    grid_search_parameters['tfidf__smooth_idf'] = (True, False)\n",
    "    grid_search_parameters['tfidf__sublinear_tf'] = (True, False)\n",
    "\n",
    "    list_tfidf_vect_pipe_and_features.append(('vect', CountVectorizer()))\n",
    "    list_tfidf_vect_pipe_and_features.append(grid_search_parameters)\n",
    "\n",
    "    return(list_tfidf_vect_pipe_and_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe_spam_ham_features []\n",
      "grid_search_parameters {}\n",
      "pipe_spam_ham_features [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None))]\n",
      "grid_search_parameters {'vect__stop_words': ('english', None), 'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], 'vect__max_df': (0.9, 1), 'vect__lowercase': (True, False), 'vect__binary': (True, False)}\n",
      "pipe_spam_ham_features []\n",
      "grid_search_parameters {}\n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/root/anaconda3/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/root/anaconda3/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f59293dedb0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/root/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/root/anacon.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f59293dedb0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/root/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/root/anacon.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    122         except (RuntimeError, AssertionError):\n    123             old_loop = None\n    124         try:\n    125             self._setup_logging()\n    126             asyncio.set_event_loop(self.asyncio_loop)\n--> 127             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    128         finally:\n    129             asyncio.set_event_loop(old_loop)\n    130 \n    131     def stop(self):\n\n...........................................................................\n/root/anaconda3/lib/python3.6/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    417             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    418                                    finalizer=self._asyncgen_finalizer_hook)\n    419         try:\n    420             events._set_running_loop(self)\n    421             while True:\n--> 422                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    423                 if self._stopping:\n    424                     break\n    425         finally:\n    426             self._stopping = False\n\n...........................................................................\n/root/anaconda3/lib/python3.6/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1427                         logger.warning('Executing %s took %.3f seconds',\n   1428                                        _format_handle(handle), dt)\n   1429                 finally:\n   1430                     self._current_handle = None\n   1431             else:\n-> 1432                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(11, 1)>>\n   1433         handle = None  # Needed to break cycles when an exception occurs.\n   1434 \n   1435     def _set_coroutine_wrapper(self, enabled):\n   1436         try:\n\n...........................................................................\n/root/anaconda3/lib/python3.6/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(11, 1)>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (11, 1)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=11, events=1)\n    112             self.writers.remove(fd)\n    113         del self.handlers[fd]\n    114 \n    115     def _handle_events(self, fd, events):\n    116         fileobj, handler_func = self.handlers[fd]\n--> 117         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    118 \n    119     def start(self):\n    120         try:\n    121             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 7, 24, 23, 0, 35, 839475, tzinfo=tzutc()), 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'session': '2d55fa7e03144fe881c23dc745378a31', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'2d55fa7e03144fe881c23dc745378a31']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 7, 24, 23, 0, 35, 839475, tzinfo=tzutc()), 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'session': '2d55fa7e03144fe881c23dc745378a31', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'2d55fa7e03144fe881c23dc745378a31'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 7, 24, 23, 0, 35, 839475, tzinfo=tzutc()), 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'session': '2d55fa7e03144fe881c23dc745378a31', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\"\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\",), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\",)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", store_history=True, silent=False, shell_futures=True)\n   2657         -------\n   2658         result : :class:`ExecutionResult`\n   2659         \"\"\"\n   2660         try:\n   2661             result = self._run_cell(\n-> 2662                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\"\n        store_history = True\n        silent = False\n        shell_futures = True\n   2663         finally:\n   2664             self.events.trigger('post_execute')\n   2665             if not silent:\n   2666                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", store_history=True, silent=False, shell_futures=True)\n   2780                 self.displayhook.exec_result = result\n   2781 \n   2782                 # Execute the user code\n   2783                 interactivity = 'none' if silent else self.ast_node_interactivity\n   2784                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2785                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2786                 \n   2787                 self.last_execution_succeeded = not has_raised\n   2788                 self.last_execution_result = result\n   2789 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.For object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.For object>, <_ast.For object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Expr object>, ...], cell_name='<ipython-input-52-1318c57ad5a7>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f58d47aacc0, executi...rue silent=False shell_futures=True> result=None>)\n   2898 \n   2899         try:\n   2900             for i, node in enumerate(to_run_exec):\n   2901                 mod = ast.Module([node])\n   2902                 code = compiler(mod, cell_name, \"exec\")\n-> 2903                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f58d47ace40, file \"<ipython-input-52-1318c57ad5a7>\", line 90>\n        result = <ExecutionResult object at 7f58d47aacc0, executi...rue silent=False shell_futures=True> result=None>\n   2904                     return True\n   2905 \n   2906             for i, node in enumerate(to_run_interactive):\n   2907                 mod = ast.Interactive([node])\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f58d47ace40, file \"<ipython-input-52-1318c57ad5a7>\", line 90>, result=<ExecutionResult object at 7f58d47aacc0, executi...rue silent=False shell_futures=True> result=None>)\n   2958         outflag = True  # happens in more places, so it's easier as default\n   2959         try:\n   2960             try:\n   2961                 self.hooks.pre_run_code_hook()\n   2962                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2963                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f58d47ace40, file \"<ipython-input-52-1318c57ad5a7>\", line 90>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'ES_Metric_Insert': <function ES_Metric_Insert>, 'Elasticsearch': <class 'elasticsearch.client.Elasticsearch'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', 'import numpy as np\\nimport matplotlib.pyplot as p...klearn.ensemble import GradientBoostingClassifier', 'raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",s...ption(\\'display.max_colwidth\\',100)\\nraw_data.head()', \"print(raw_data.shape)\\npd.crosstab(raw_data['label'],columns = 'label',normalize=True)\", '# Create Test Train Fit\\n\\n# Define X and y.\\nX = r...test_split(X, y, random_state=99, test_size= 0.3)', \"# Calculate null accuracy.\\ny_test_binary = np.wh...\\nprint('Percent Spam:', 1 - y_test_binary.mean())\", 'class LemmaTokenizer(object):\\n    def __init__(s...nl.lemmatize(t) for t in word_tokenize(articles)]', 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'es.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'db.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'es = Elasticsearch()\\nes.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'ES_Metric_Insert': <function ES_Metric_Insert>, 'Elasticsearch': <class 'elasticsearch.client.Elasticsearch'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', 'import numpy as np\\nimport matplotlib.pyplot as p...klearn.ensemble import GradientBoostingClassifier', 'raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",s...ption(\\'display.max_colwidth\\',100)\\nraw_data.head()', \"print(raw_data.shape)\\npd.crosstab(raw_data['label'],columns = 'label',normalize=True)\", '# Create Test Train Fit\\n\\n# Define X and y.\\nX = r...test_split(X, y, random_state=99, test_size= 0.3)', \"# Calculate null accuracy.\\ny_test_binary = np.wh...\\nprint('Percent Spam:', 1 - y_test_binary.mean())\", 'class LemmaTokenizer(object):\\n    def __init__(s...nl.lemmatize(t) for t in word_tokenize(articles)]', 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'es.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'db.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'es = Elasticsearch()\\nes.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, ...}\n   2964             finally:\n   2965                 # Reset our crash handler in place\n   2966                 sys.excepthook = old_excepthook\n   2967         except SystemExit as e:\n\n...........................................................................\n/root/anaconda3/bin/jupyter_files/session_20/spamfilter/<ipython-input-52-1318c57ad5a7> in <module>()\n     92     ## Adding the classifier to be used\n     93     pipe_spam_ham.set_params(clf = classifier)\n     94     \n     95     gs_clf_pipe_spam_ham = GridSearchCV(pipe_spam_ham, grid_search_parameters, n_jobs=-1, cv=cv_value, return_train_score=True)\n     96     \n---> 97     ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test,grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier)\n     98     \n     99     \n    100 \n    101 ##                          'vect__analyzer': (‘word’, ‘char’, ‘char_wb’),\n\n...........................................................................\n/root/anaconda3/bin/jupyter_files/session_20/spamfilter/<ipython-input-28-ec23a0d7533f> in ML_Pipeline_Processing_And_Metrics(run_id=889296, X_train=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y_train=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, X_test=1661                                            ...gins w...\nName: text, Length: 1672, dtype: object, y_test=1661     ham\n2718     ham\n2409     ham\n2385     ...     ham\nName: label, Length: 1672, dtype: object, grid_search_parameters={'vect__stop_words': ('english', None)}, gs_clf_pipe_spam_ham=GridSearchCV(cv=2, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=0), cv_value=2, classifier_name=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))\n      1 def ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name):\n      2     \n----> 3     gs_clf_pipe_spam_ham.fit(X_train, y_train)\n      4 \n      5     ## Find predictions for the pipeline\n      6     y_pred = gs_clf_pipe_spam_ham.predict(X_test)\n      7     \n      8     ## Find score of predictions\n      9     score_pipe_spam_ham = gs_clf_pipe_spam_ham.score(X_test, y_test) * 100 \n     10     \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=2, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=0), X=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=2, random_state=None, shuffle=False)>\n        X = 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object\n        y = 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Tue Jul 24 16:00:36 2018\nPID: 26084                         Python 3.6.5: /root/anaconda3/bin/python\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([1949, 1950, 1952, ..., 3897, 3898, 3899]), test=array([   0,    1,    2, ..., 1948, 1951, 1953]), verbose=0, parameters={'vect__stop_words': 'english'}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        parameters = {'vect__stop_words': 'english'}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **kwargs={'vect__stop_words': 'english'})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        kwargs = {'vect__stop_words': 'english'}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), attr='steps', **params={'vect__stop_words': 'english'})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        params = {'vect__stop_words': 'english'}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **params={'vect__stop_words': 'english'})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 444, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\", line 142, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\", line 49, in _set_params\n    super(_BaseComposition, self).set_params(**params)\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/base.py\", line 274, in set_params\n    (key, self))\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]). Check the list of available parameters with `estimator.get_params().keys()`.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Tue Jul 24 16:00:36 2018\nPID: 26084                         Python 3.6.5: /root/anaconda3/bin/python\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([1949, 1950, 1952, ..., 3897, 3898, 3899]), test=array([   0,    1,    2, ..., 1948, 1951, 1953]), verbose=0, parameters={'vect__stop_words': 'english'}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        parameters = {'vect__stop_words': 'english'}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **kwargs={'vect__stop_words': 'english'})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        kwargs = {'vect__stop_words': 'english'}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), attr='steps', **params={'vect__stop_words': 'english'})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        params = {'vect__stop_words': 'english'}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **params={'vect__stop_words': 'english'})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Tue Jul 24 16:00:36 2018\nPID: 26084                         Python 3.6.5: /root/anaconda3/bin/python\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([1949, 1950, 1952, ..., 3897, 3898, 3899]), test=array([   0,    1,    2, ..., 1948, 1951, 1953]), verbose=0, parameters={'vect__stop_words': 'english'}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        parameters = {'vect__stop_words': 'english'}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **kwargs={'vect__stop_words': 'english'})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        kwargs = {'vect__stop_words': 'english'}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), attr='steps', **params={'vect__stop_words': 'english'})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        params = {'vect__stop_words': 'english'}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **params={'vect__stop_words': 'english'})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-1318c57ad5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mgs_clf_pipe_spam_ham\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_spam_ham\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mML_Pipeline_Processing_And_Metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid_search_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs_clf_pipe_spam_ham\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-ec23a0d7533f>\u001b[0m in \u001b[0;36mML_Pipeline_Processing_And_Metrics\u001b[0;34m(run_id, X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mML_Pipeline_Processing_And_Metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs_clf_pipe_spam_ham\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgs_clf_pipe_spam_ham\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## Find predictions for the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/root/anaconda3/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/root/anaconda3/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f59293dedb0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/root/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/root/anacon.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f59293dedb0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/root/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/root/anacon.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    122         except (RuntimeError, AssertionError):\n    123             old_loop = None\n    124         try:\n    125             self._setup_logging()\n    126             asyncio.set_event_loop(self.asyncio_loop)\n--> 127             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    128         finally:\n    129             asyncio.set_event_loop(old_loop)\n    130 \n    131     def stop(self):\n\n...........................................................................\n/root/anaconda3/lib/python3.6/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    417             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    418                                    finalizer=self._asyncgen_finalizer_hook)\n    419         try:\n    420             events._set_running_loop(self)\n    421             while True:\n--> 422                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    423                 if self._stopping:\n    424                     break\n    425         finally:\n    426             self._stopping = False\n\n...........................................................................\n/root/anaconda3/lib/python3.6/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1427                         logger.warning('Executing %s took %.3f seconds',\n   1428                                        _format_handle(handle), dt)\n   1429                 finally:\n   1430                     self._current_handle = None\n   1431             else:\n-> 1432                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(11, 1)>>\n   1433         handle = None  # Needed to break cycles when an exception occurs.\n   1434 \n   1435     def _set_coroutine_wrapper(self, enabled):\n   1436         try:\n\n...........................................................................\n/root/anaconda3/lib/python3.6/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(11, 1)>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (11, 1)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=11, events=1)\n    112             self.writers.remove(fd)\n    113         del self.handlers[fd]\n    114 \n    115     def _handle_events(self, fd, events):\n    116         fileobj, handler_func = self.handlers[fd]\n--> 117         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    118 \n    119     def start(self):\n    120         try:\n    121             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 7, 24, 23, 0, 35, 839475, tzinfo=tzutc()), 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'session': '2d55fa7e03144fe881c23dc745378a31', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'2d55fa7e03144fe881c23dc745378a31']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 7, 24, 23, 0, 35, 839475, tzinfo=tzutc()), 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'session': '2d55fa7e03144fe881c23dc745378a31', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'2d55fa7e03144fe881c23dc745378a31'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 7, 24, 23, 0, 35, 839475, tzinfo=tzutc()), 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'session': '2d55fa7e03144fe881c23dc745378a31', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '9eab93e61d234f1a89c74ea2242b8c90', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\"\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\",), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\",)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", store_history=True, silent=False, shell_futures=True)\n   2657         -------\n   2658         result : :class:`ExecutionResult`\n   2659         \"\"\"\n   2660         try:\n   2661             result = self._run_cell(\n-> 2662                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\"\n        store_history = True\n        silent = False\n        shell_futures = True\n   2663         finally:\n   2664             self.events.trigger('post_execute')\n   2665             if not silent:\n   2666                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"pipe_spam_ham = []\\npipe_spam_ham_features = []\\ng...   'vect__tokenizer: (LemmaTokenizer()),\\n       \\n\", store_history=True, silent=False, shell_futures=True)\n   2780                 self.displayhook.exec_result = result\n   2781 \n   2782                 # Execute the user code\n   2783                 interactivity = 'none' if silent else self.ast_node_interactivity\n   2784                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2785                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2786                 \n   2787                 self.last_execution_succeeded = not has_raised\n   2788                 self.last_execution_result = result\n   2789 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.For object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.For object>, <_ast.For object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Expr object>, ...], cell_name='<ipython-input-52-1318c57ad5a7>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f58d47aacc0, executi...rue silent=False shell_futures=True> result=None>)\n   2898 \n   2899         try:\n   2900             for i, node in enumerate(to_run_exec):\n   2901                 mod = ast.Module([node])\n   2902                 code = compiler(mod, cell_name, \"exec\")\n-> 2903                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f58d47ace40, file \"<ipython-input-52-1318c57ad5a7>\", line 90>\n        result = <ExecutionResult object at 7f58d47aacc0, executi...rue silent=False shell_futures=True> result=None>\n   2904                     return True\n   2905 \n   2906             for i, node in enumerate(to_run_interactive):\n   2907                 mod = ast.Interactive([node])\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f58d47ace40, file \"<ipython-input-52-1318c57ad5a7>\", line 90>, result=<ExecutionResult object at 7f58d47aacc0, executi...rue silent=False shell_futures=True> result=None>)\n   2958         outflag = True  # happens in more places, so it's easier as default\n   2959         try:\n   2960             try:\n   2961                 self.hooks.pre_run_code_hook()\n   2962                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2963                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f58d47ace40, file \"<ipython-input-52-1318c57ad5a7>\", line 90>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'ES_Metric_Insert': <function ES_Metric_Insert>, 'Elasticsearch': <class 'elasticsearch.client.Elasticsearch'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', 'import numpy as np\\nimport matplotlib.pyplot as p...klearn.ensemble import GradientBoostingClassifier', 'raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",s...ption(\\'display.max_colwidth\\',100)\\nraw_data.head()', \"print(raw_data.shape)\\npd.crosstab(raw_data['label'],columns = 'label',normalize=True)\", '# Create Test Train Fit\\n\\n# Define X and y.\\nX = r...test_split(X, y, random_state=99, test_size= 0.3)', \"# Calculate null accuracy.\\ny_test_binary = np.wh...\\nprint('Percent Spam:', 1 - y_test_binary.mean())\", 'class LemmaTokenizer(object):\\n    def __init__(s...nl.lemmatize(t) for t in word_tokenize(articles)]', 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'es.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'db.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'es = Elasticsearch()\\nes.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'ES_Metric_Insert': <function ES_Metric_Insert>, 'Elasticsearch': <class 'elasticsearch.client.Elasticsearch'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', 'import numpy as np\\nimport matplotlib.pyplot as p...klearn.ensemble import GradientBoostingClassifier', 'raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",s...ption(\\'display.max_colwidth\\',100)\\nraw_data.head()', \"print(raw_data.shape)\\npd.crosstab(raw_data['label'],columns = 'label',normalize=True)\", '# Create Test Train Fit\\n\\n# Define X and y.\\nX = r...test_split(X, y, random_state=99, test_size= 0.3)', \"# Calculate null accuracy.\\ny_test_binary = np.wh...\\nprint('Percent Spam:', 1 - y_test_binary.mean())\", 'class LemmaTokenizer(object):\\n    def __init__(s...nl.lemmatize(t) for t in word_tokenize(articles)]', 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'def calculate_metrics_push_to_es(run_id_insert, ...e_insert,weighted_score_insert)\\n    \\n    return()', 'def ES_Metric_Insert(run_id_insert,algorithm_nam...sh(index=\"ml-performance-metrics\")\\n\\n\\n    return()', 'def ML_Pipeline_Processing_And_Metrics(run_id,X_..._means_std, y_test,y_pred)\\n    \\n    \\n    return()', \"pipe_spam_ham = []\\npipe_spam_ham_features = []\\n\\n...    'vect__tokenizer: (LemmaTokenizer()),\\n       \", 'es.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'db.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', 'es = Elasticsearch()\\nes.delete(index=\"ml-performance-metrics\",doc_type=\"text\",run_id=100738)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, ...}\n   2964             finally:\n   2965                 # Reset our crash handler in place\n   2966                 sys.excepthook = old_excepthook\n   2967         except SystemExit as e:\n\n...........................................................................\n/root/anaconda3/bin/jupyter_files/session_20/spamfilter/<ipython-input-52-1318c57ad5a7> in <module>()\n     92     ## Adding the classifier to be used\n     93     pipe_spam_ham.set_params(clf = classifier)\n     94     \n     95     gs_clf_pipe_spam_ham = GridSearchCV(pipe_spam_ham, grid_search_parameters, n_jobs=-1, cv=cv_value, return_train_score=True)\n     96     \n---> 97     ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test,grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier)\n     98     \n     99     \n    100 \n    101 ##                          'vect__analyzer': (‘word’, ‘char’, ‘char_wb’),\n\n...........................................................................\n/root/anaconda3/bin/jupyter_files/session_20/spamfilter/<ipython-input-28-ec23a0d7533f> in ML_Pipeline_Processing_And_Metrics(run_id=889296, X_train=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y_train=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, X_test=1661                                            ...gins w...\nName: text, Length: 1672, dtype: object, y_test=1661     ham\n2718     ham\n2409     ham\n2385     ...     ham\nName: label, Length: 1672, dtype: object, grid_search_parameters={'vect__stop_words': ('english', None)}, gs_clf_pipe_spam_ham=GridSearchCV(cv=2, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=0), cv_value=2, classifier_name=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))\n      1 def ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name):\n      2     \n----> 3     gs_clf_pipe_spam_ham.fit(X_train, y_train)\n      4 \n      5     ## Find predictions for the pipeline\n      6     y_pred = gs_clf_pipe_spam_ham.predict(X_test)\n      7     \n      8     ## Find score of predictions\n      9     score_pipe_spam_ham = gs_clf_pipe_spam_ham.score(X_test, y_test) * 100 \n     10     \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=2, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=0), X=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=2, random_state=None, shuffle=False)>\n        X = 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object\n        y = 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Tue Jul 24 16:00:36 2018\nPID: 26084                         Python 3.6.5: /root/anaconda3/bin/python\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), 3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, 3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, {'score': <function _passthrough_scorer>}, array([1949, 1950, 1952, ..., 3897, 3898, 3899]), array([   0,    1,    2, ..., 1948, 1951, 1953]), 0, {'vect__stop_words': 'english'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=3032         gonna let me know cos comes bak fro...t offense\nName: text, Length: 3900, dtype: object, y=3032     ham\n3381     ham\n4805     ham\n504      ...     ham\nName: label, Length: 3900, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([1949, 1950, 1952, ..., 3897, 3898, 3899]), test=array([   0,    1,    2, ..., 1948, 1951, 1953]), verbose=0, parameters={'vect__stop_words': 'english'}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        parameters = {'vect__stop_words': 'english'}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **kwargs={'vect__stop_words': 'english'})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        kwargs = {'vect__stop_words': 'english'}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), attr='steps', **params={'vect__stop_words': 'english'})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        params = {'vect__stop_words': 'english'}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/root/anaconda3/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))]), **params={'vect__stop_words': 'english'})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...B(alpha=1.0, class_prior=None, fit_prior=True))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "pipe_spam_ham = []\n",
    "pipe_spam_ham_features = []\n",
    "grid_search_parameters = {}\n",
    "\n",
    "run_id = randint(100000, 999999)\n",
    "\n",
    "## Cross_Val value\n",
    "cv_value = 2\n",
    "\n",
    "# Define 10 fold cross-validation\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "\n",
    "## Addition of Count Vectorizer\n",
    "list_count_vect_pipe_and_features = add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "pipe_spam_ham_features.append(list_count_vect_pipe_and_features[0])\n",
    "for keys in list_count_vect_pipe_and_features[1]:\n",
    "    grid_search_parameters[keys] = list_count_vect_pipe_and_features[1][keys] \n",
    "\n",
    "\n",
    "## Remove Count Vectorizer\n",
    "\n",
    "\n",
    "for key in grid_search_parameters.copy():\n",
    "     if 'vect' in key.lower():\n",
    "        del grid_search_parameters[key]\n",
    "\n",
    "for item in pipe_spam_ham_features:\n",
    "    if 'vect' in item:\n",
    "        pipe_spam_ham_features.remove(item)\n",
    "        \n",
    "        \n",
    "## Addition of TfIdf Vectorizer\n",
    "\n",
    "\n",
    "list_tfidf_vect_pipe_and_features = add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "pipe_spam_ham_features.append(list_tfidf_vect_pipe_and_features[0])\n",
    "for keys in list_tfidf_vect_pipe_and_features[1]:\n",
    "    grid_search_parameters[keys] = list_tfidf_vect_pipe_and_features[1][keys] \n",
    "\n",
    "## Remove tfidf Vectorizer\n",
    "for key in grid_search_parameters.copy():\n",
    "     if 'tfidf' in key.lower():\n",
    "        del grid_search_parameters[key]\n",
    "\n",
    "for item in pipe_spam_ham_features:\n",
    "    if 'tfidf' in item:\n",
    "        pipe_spam_ham_features.remove(item)\n",
    "\n",
    "\n",
    "#pipe_spam_ham_features.append(('hash', HashingVectorizer()))\n",
    "\n",
    "\n",
    "\n",
    "## Initializing the classifier to Naieve Bayes\n",
    "pipe_spam_ham_features.append(('clf', MultinomialNB()))\n",
    "\n",
    "## Putting together the various classification algorithms to use\n",
    "clfs = []\n",
    "clfs.append(MultinomialNB())\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=5))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "clfs.append(LogisticRegression())\n",
    "\n",
    "\n",
    "## Setting up the pipeline\n",
    "pipe_spam_ham = Pipeline(pipe_spam_ham_features)\n",
    "\n",
    "\n",
    "\n",
    "## Trying out the various possible classifiers:\n",
    "\n",
    "for classifier in clfs:\n",
    "    \n",
    "    ## Adding the classifier to be used\n",
    "    pipe_spam_ham.set_params(clf = classifier)\n",
    "    \n",
    "    gs_clf_pipe_spam_ham = GridSearchCV(pipe_spam_ham, grid_search_parameters, n_jobs=-1, cv=cv_value, return_train_score=True)\n",
    "    \n",
    "    ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test,grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier)\n",
    "    \n",
    "    \n",
    "\n",
    "##                          'vect__analyzer': (‘word’, ‘char’, ‘char_wb’),\n",
    "##                          'vect__preprocessor: ('callable','None'),\n",
    "##                          'vect__tokenizer: (LemmaTokenizer()),\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
