{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import pytz\n",
    "import graphviz\n",
    "import copy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from random import randint\n",
    "\n",
    "## Elastic Search for Metrics\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB         \n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# KNN Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Gradient Booster Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading file and looking into the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",sep='\\t',names=['label','text'])\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     label\n",
       "label          \n",
       "ham    0.865937\n",
       "spam   0.134063"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "pd.crosstab(raw_data['label'],columns = 'label',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape (3900, 7234)\n",
      "['yet', 'yetty', 'yetunde', 'yi', 'yijue', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yor', 'yorge', 'you', 'youdoing', 'youi', 'young', 'younger', 'youphone', 'your', 'youre', 'yourinclusive', 'yourjob', 'yours', 'yourself', 'youuuuu', 'youwanna', 'yowifes', 'yr', 'yrs', 'ystrday', 'yummmm', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'zaher', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'Ã¨n']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "print('X_train Shape', X_train_dtm.shape)\n",
    "\n",
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks like we have 7234 Vectors after Count Vectorizer. From 3900 lines of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1179)\t1\n",
      "  (0, 3379)\t1\n",
      "  (0, 3605)\t1\n",
      "  (0, 4374)\t1\n",
      "  (0, 4481)\t1\n",
      "  (0, 5611)\t1\n",
      "  (0, 6317)\t1\n",
      "  (0, 6477)\t1\n",
      "  (0, 7196)\t2\n",
      "  (1, 4598)\t1\n",
      "  (1, 6359)\t1\n",
      "  (2, 1172)\t1\n",
      "  (2, 2031)\t1\n",
      "  (2, 5225)\t1\n",
      "  (2, 6382)\t1\n",
      "  (2, 6985)\t1\n",
      "  (2, 6988)\t1\n",
      "  (2, 7017)\t1\n",
      "  (2, 7196)\t1\n",
      "  (3, 1486)\t1\n",
      "  (3, 3749)\t1\n",
      "  (3, 3872)\t1\n",
      "  (3, 5913)\t1\n",
      "  (4, 677)\t1\n",
      "  (4, 1021)\t1\n",
      "  :\t:\n",
      "  (1669, 3476)\t1\n",
      "  (1669, 3487)\t1\n",
      "  (1669, 4588)\t1\n",
      "  (1669, 6254)\t1\n",
      "  (1669, 6302)\t1\n",
      "  (1669, 6364)\t1\n",
      "  (1670, 868)\t1\n",
      "  (1670, 989)\t1\n",
      "  (1670, 1508)\t1\n",
      "  (1670, 4109)\t1\n",
      "  (1670, 6564)\t1\n",
      "  (1671, 875)\t1\n",
      "  (1671, 1205)\t1\n",
      "  (1671, 1295)\t1\n",
      "  (1671, 1516)\t1\n",
      "  (1671, 2734)\t1\n",
      "  (1671, 2916)\t1\n",
      "  (1671, 3151)\t1\n",
      "  (1671, 3352)\t1\n",
      "  (1671, 3493)\t1\n",
      "  (1671, 4313)\t1\n",
      "  (1671, 4364)\t1\n",
      "  (1671, 5939)\t1\n",
      "  (1671, 7046)\t1\n",
      "  (1671, 7196)\t1\n"
     ]
    }
   ],
   "source": [
    "## Vocabulary used:\n",
    "# vect.vocabulary_\n",
    "\n",
    "print(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Train Fit\n",
    "\n",
    "# Define X and y.\n",
    "X = raw_data.text\n",
    "y = raw_data.label\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Ham: 0.8624401913875598\n",
      "Percent Spam: 0.13755980861244022\n"
     ]
    }
   ],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test=='ham', 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent Ham:', y_test_binary.mean())\n",
    "print('Percent Spam:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to cleanup the data through pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics and Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score,test_scores_csv_means_std, y_test,y_pred):\n",
    "\n",
    "    macro_score = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    micro_score = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    weighted_score = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "  \n",
    "\n",
    "    macro_score_insert = {'macro_precision': macro_score[0] * 100, 'macro_recall': macro_score[1]  * 100, 'macro_fscore':macro_score[2]  * 100}\n",
    "    micro_score_insert = {'micro_precision': micro_score[0] * 100, 'micro_recall': micro_score[1] * 100, 'micro_fscore':micro_score[2] * 100}\n",
    "    weighted_score_insert = {'weighted_precision': weighted_score[0] * 100, 'weighted_recall': weighted_score[1] * 100, 'weighted_fscore':weighted_score[2] * 100}\n",
    "    score_insert = {'score': score}\n",
    "    \n",
    "    print(score_insert)\n",
    "    \n",
    "    ## Print Accuracy of the current Test\n",
    "    print(algorithm_name_insert , ' pipeline test accuracy: %.3f' % score)\n",
    "    \n",
    "    ## Push the data to ElasticSearch\n",
    "\n",
    "    ES_Metric_Insert(run_id_insert, algorithm_name_insert, test_parameters_insert,gs_best_parameters_pipe_spam_ham, score_insert,test_scores_csv_means_std, macro_score_insert,micro_score_insert,weighted_score_insert)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Data into Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_Metric_Insert(run_id_insert,algorithm_name, test_parameters, gs_best_parameters_pipe_spam_ham, score, test_scores_csv_means_std, macro_scores, micro_scores, weighted_scores):\n",
    "    es = Elasticsearch()\n",
    "    \n",
    "    final_dict = {}\n",
    "    \n",
    "    my_current_time = datetime.now(tz=pytz.utc)\n",
    "    timestamp_insert = {'timestamp': my_current_time}\n",
    "    author_insert = {'author': 'Rahul'}\n",
    "    final_dict.update(run_id_insert)\n",
    "    final_dict.update(timestamp_insert)\n",
    "    final_dict.update(author_insert)\n",
    "    final_dict.update(algorithm_name)\n",
    "    final_dict.update(test_parameters)\n",
    "    final_dict.update(gs_best_parameters_pipe_spam_ham)\n",
    "    final_dict.update(score)\n",
    "    final_dict.update(test_scores_csv_means_std)\n",
    "    final_dict.update(macro_scores)\n",
    "    final_dict.update(micro_scores)\n",
    "    final_dict.update(weighted_scores)\n",
    "        \n",
    "    res = es.index(index=\"ml-performance-metrics\", doc_type='text', body=final_dict)\n",
    "    es.indices.refresh(index=\"ml-performance-metrics\")\n",
    "\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the ML Pipeline and Calculate Metrics (using another function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test, grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier_name):\n",
    "    \n",
    "    gs_clf_pipe_spam_ham.fit(X_train, y_train)\n",
    "\n",
    "    ## Find predictions for the pipeline\n",
    "    y_pred = gs_clf_pipe_spam_ham.predict(X_test)\n",
    "    \n",
    "    ## Find score of predictions\n",
    "    score_pipe_spam_ham = gs_clf_pipe_spam_ham.score(X_test, y_test) * 100 \n",
    "    \n",
    "    ## Best Grid Search Parameters selected for this case    \n",
    "    gs_best_parameters_pipe_spam_ham = {}\n",
    "    for param_name in sorted(grid_search_parameters.keys()):\n",
    "        if param_name == 'vect__tokenizer':\n",
    "            gs_best_parameters_pipe_spam_ham[param_name] = 'LemmaTokenizer'\n",
    "        else:\n",
    "            gs_best_parameters_pipe_spam_ham[param_name] = gs_clf_pipe_spam_ham.best_params_[param_name]\n",
    "        \n",
    "    \n",
    "    ## Setting up for reporting to Screen and ElasticSearch\n",
    "    \n",
    "    ## Add Run Id for each run. This helps with fishing out the correct dataset in cloud\n",
    "    run_id_insert = {'run_id' : run_id}\n",
    "    \n",
    "    ## Save Classifier name as a string\n",
    "    \n",
    "    classifier_string = str(classifier_name)\n",
    "    classifer_name_only = classifier_string.split(\"(\")[0]\n",
    "    \n",
    "    algorithm_name_insert = {'Algorithm_Name' : classifer_name_only}\n",
    "    \n",
    "    ## Add Classifier Parameters to output\n",
    "    test_parameters_insert = {'test_parameters' : str(pipe_spam_ham)}\n",
    "    \n",
    "    \n",
    "    ## Breaking test cv scores and calculating mean and standard Deviation of each.\n",
    "    cv_scores_df = pd.DataFrame.from_dict(gs_clf_pipe_spam_ham.cv_results_)\n",
    "    \n",
    "    test_scores_csv_means_std = {}\n",
    "    \n",
    "    test_scores_csv_means_std['mean_fit_time'] = cv_scores_df.loc[0 ,'mean_fit_time']\n",
    "    test_scores_csv_means_std['std_fit_time'] = cv_scores_df.loc[0 ,'std_fit_time']\n",
    "    test_scores_csv_means_std['mean_test_score'] = cv_scores_df.loc[0 ,'mean_test_score'] * 100\n",
    "    test_scores_csv_means_std['std_test_score'] = cv_scores_df.loc[0 ,'std_test_score']\n",
    "    test_scores_csv_means_std['mean_train_score'] = cv_scores_df.loc[0 ,'mean_train_score']  * 100\n",
    "    test_scores_csv_means_std['std_train_score'] = cv_scores_df.loc[0 ,'std_train_score']\n",
    "    \n",
    "\n",
    "    ## Send all the collected data to the metric collection and ES insert system.\n",
    "    calculate_metrics_push_to_es(run_id_insert, algorithm_name_insert, test_parameters_insert, gs_best_parameters_pipe_spam_ham, score_pipe_spam_ham, test_scores_csv_means_std, y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Vectorizers and ML Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vectorizer_ml_algo(vector_ml_keyword):\n",
    "    \n",
    "    ## Remove from gridsearch\n",
    "    for key in grid_search_parameters.copy():\n",
    "         if vector_ml_keyword in key.lower():\n",
    "            del grid_search_parameters[key]\n",
    "    \n",
    "    \n",
    "    ## Remove from spam ham pipeline\n",
    "    \n",
    "    for item in pipe_spam_ham_features:\n",
    "        if vector_ml_keyword in item:\n",
    "            pipe_spam_ham_features.remove(item)\n",
    "    \n",
    "    return()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Count Vectorizer and associated Features for Testing\n",
    "def add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters):\n",
    "\n",
    "    grid_search_parameters['vect__binary'] = (False,True)\n",
    "    grid_search_parameters['vect__lowercase'] = (True,False)\n",
    "    grid_search_parameters['vect__tokenizer'] = (LemmaTokenizer(),None)\n",
    "    \n",
    "##    Grid Search Parameters avialable for testing. After initial tests it looks like the above params work best. So using those. \n",
    "#     grid_search_parameters['vect__stop_words'] = ('english',None)\n",
    "#     grid_search_parameters['vect__ngram_range'] = [(1, 1),(1, 2),(1, 3), (1, 4)]\n",
    "#     grid_search_parameters['vect__max_df'] = (0.9,1)\n",
    "#     grid_search_parameters['vect__lowercase'] = (True, False)\n",
    "#     grid_search_parameters['vect__binary'] = (True, False)\n",
    "#     grid_search_parameters['vect__tokenizer'] = (LemmaTokenizer())\n",
    "#     grid_search_parameters['vect__min_df'] = (5,10)\n",
    "    pipe_spam_ham_features.append(('vect', CountVectorizer()))\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tf-Idf Vectorizer and associated Features for Testing\n",
    "def add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['tfidf__norm'] = ('l2','l1')\n",
    "    grid_search_parameters['tfidf__smooth_idf'] = (True,False)\n",
    "    \n",
    "#     ## Grid Search Parameters avialable for testing. After initial tests it looks like the above params work best. So using those.\n",
    "#     grid_search_parameters['tfidf__use_idf'] = (True, False)\n",
    "#     grid_search_parameters['tfidf__norm'] = ('l1','l2','max')\n",
    "#     grid_search_parameters['tfidf__smooth_idf'] = (True, False)\n",
    "#     grid_search_parameters['tfidf__sublinear_tf'] = (True, False)\n",
    "\n",
    "    pipe_spam_ham_features.append(('tfidf', TfidfVectorizer()))\n",
    "    \n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tf-Idf Vectorizer and associated Features for Testing\n",
    "def add_TruncatedSVD(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['truncatedsvd__n_components'] = (500, 400, 200)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('truncatedsvd', TruncatedSVD()))\n",
    "    \n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Naive Bayes Algorithm\n",
    "def add_multinomialNB(pipe_spam_ham_features,grid_search_parameters):\n",
    "\n",
    "    grid_search_parameters['nb__alpha'] = (1,0.9)\n",
    "    grid_search_parameters['nb__fit_prior'] = (True,False)\n",
    "#     ## Grid Search Parameters avialable for testing. After initial tests it looks like the above params work best. So using those.    \n",
    "#     grid_search_parameters['nb__alpha'] = (0,1)\n",
    "#     grid_search_parameters['nb__fit_prior'] = (True, False)\n",
    "    \n",
    "    \n",
    "    pipe_spam_ham_features.append(('nb', MultinomialNB()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Naive Bayes Algorithm\n",
    "def add_knn(pipe_spam_ham_features,grid_search_parameters):\n",
    "\n",
    "    grid_search_parameters['knn__n_neighbors'] = (1,2,3,4,5,6,7,8,9,10)\n",
    "    grid_search_parameters['knn__weights'] = ('uniform', 'distance')\n",
    "    #grid_search_parameters['knn__algorithm'] = ('ball_tree', 'kd_tree')\n",
    "    \n",
    "    pipe_spam_ham_features.append(('knn', KNeighborsClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Random Forest Algorithm\n",
    "def add_randomforest(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['rf__n_estimators'] = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)\n",
    "    grid_search_parameters['rf__max_depth'] = (10,100,1000,None)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('rf', RandomForestClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Logistic Regression Algorithm\n",
    "def add_logistic_regression(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['lr__penalty'] = ('l1','l2')\n",
    "    \n",
    "    pipe_spam_ham_features.append(('lr', LogisticRegression()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add SVC Algorithm\n",
    "def add_svc_regression(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['svc__C'] = (1.0,0.9,0.8)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('svc', SVC()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add GradientBoostingClassifier Algorithm\n",
    "def add_gradient_boosting_classifer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['gbc__n_estimators'] = (100,200,300,1000)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('gbc', GradientBoostingClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add DecisionTreeClassifier Algorithm\n",
    "def add_decisiontree_classifer(pipe_spam_ham_features,grid_search_parameters):\n",
    "    \n",
    "    grid_search_parameters['dtc__max_depth'] = (10,100,1000,None)\n",
    "    \n",
    "    pipe_spam_ham_features.append(('dtc', DecisionTreeClassifier()))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...owski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'knn__n_neighbors': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 'knn__weights': ('uniform', 'distance')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 15:48:04,897 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.010s]\n",
      "2018-07-26 15:48:04,908 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.009s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 94.79665071770334}\n",
      "{'Algorithm_Name': 'KNeighborsClassifier'}  pipeline test accuracy: 94.797\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'rf__n_estimators': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20), 'rf__max_depth': (10, 100, 1000, None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:25:41,894 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.006s]\n",
      "2018-07-26 16:25:41,903 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.008s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 96.71052631578947}\n",
      "{'Algorithm_Name': 'RandomForestClassifier'}  pipeline test accuracy: 96.711\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'lr__penalty': ('l1', 'l2')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:26:41,074 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.010s]\n",
      "2018-07-26 16:26:41,092 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.016s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 98.14593301435407}\n",
      "{'Algorithm_Name': 'LogisticRegression'}  pipeline test accuracy: 98.146\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'nb__alpha': (1, 0.9), 'nb__fit_prior': (True, False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:28:30,247 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.006s]\n",
      "2018-07-26 16:28:30,258 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.010s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 98.74401913875597}\n",
      "{'Algorithm_Name': 'MultinomialNB'}  pipeline test accuracy: 98.744\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'svc__C': (1.0, 0.9, 0.8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-07-26 16:30:32,130 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.008s]\n",
      "2018-07-26 16:30:32,140 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.008s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 86.24401913875597}\n",
      "{'Algorithm_Name': 'SVC'}  pipeline test accuracy: 86.244\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'gbc__n_estimators': (100, 200, 300, 1000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:35:58,153 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.008s]\n",
      "2018-07-26 16:35:58,171 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.015s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 97.54784688995215}\n",
      "{'Algorithm_Name': 'GradientBoostingClassifier'}  pipeline test accuracy: 97.548\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "{'vect__binary': (False, True), 'vect__lowercase': (True, False), 'vect__tokenizer': (<__main__.LemmaTokenizer object at 0x7f8038e426d8>, None), 'dtc__max_depth': (10, 100, 1000, None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:37:52,016 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.009s]\n",
      "2018-07-26 16:37:52,034 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.016s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 96.17224880382776}\n",
      "{'Algorithm_Name': 'DecisionTreeClassifier'}  pipeline test accuracy: 96.172\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...owski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'knn__n_neighbors': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 'knn__weights': ('uniform', 'distance')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:38:54,976 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.007s]\n",
      "2018-07-26 16:38:54,986 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.009s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 96.71052631578947}\n",
      "{'Algorithm_Name': 'KNeighborsClassifier'}  pipeline test accuracy: 96.711\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'rf__n_estimators': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20), 'rf__max_depth': (10, 100, 1000, None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:41:21,269 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.007s]\n",
      "2018-07-26 16:41:21,280 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.010s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 97.06937799043062}\n",
      "{'Algorithm_Name': 'RandomForestClassifier'}  pipeline test accuracy: 97.069\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'lr__penalty': ('l1', 'l2')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:41:24,234 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.006s]\n",
      "2018-07-26 16:41:24,244 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.009s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 96.77033492822966}\n",
      "{'Algorithm_Name': 'LogisticRegression'}  pipeline test accuracy: 96.770\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...True,\n",
      "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'nb__alpha': (1, 0.9), 'nb__fit_prior': (True, False)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:41:30,275 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.012s]\n",
      "2018-07-26 16:41:30,294 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.017s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 98.02631578947368}\n",
      "{'Algorithm_Name': 'MultinomialNB'}  pipeline test accuracy: 98.026\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'svc__C': (1.0, 0.9, 0.8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-07-26 16:41:49,416 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.006s]\n",
      "2018-07-26 16:41:49,427 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.009s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 86.24401913875597}\n",
      "{'Algorithm_Name': 'SVC'}  pipeline test accuracy: 86.244\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'gbc__n_estimators': (100, 200, 300, 1000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:44:44,623 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.034s]\n",
      "2018-07-26 16:44:44,644 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.018s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 97.42822966507177}\n",
      "{'Algorithm_Name': 'GradientBoostingClassifier'}  pipeline test accuracy: 97.428\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      " ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "{'tfidf__norm': ('l2', 'l1'), 'tfidf__smooth_idf': (True, False), 'dtc__max_depth': (10, 100, 1000, None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-26 16:44:53,659 : INFO : POST http://localhost:9200/ml-performance-metrics/text [status:201 request:0.007s]\n",
      "2018-07-26 16:44:53,670 : INFO : POST http://localhost:9200/ml-performance-metrics/_refresh [status:200 request:0.010s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 95.27511961722487}\n",
      "{'Algorithm_Name': 'DecisionTreeClassifier'}  pipeline test accuracy: 95.275\n"
     ]
    }
   ],
   "source": [
    "pipe_spam_ham = []\n",
    "pipe_spam_ham_features = []\n",
    "grid_search_parameters = {}\n",
    "list_ml_algo = {}\n",
    "\n",
    "\n",
    "run_id = randint(100000, 999999)\n",
    "\n",
    "## Cross_Val value\n",
    "cv_value = 2\n",
    "\n",
    "# Define 10 fold cross-validation\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "# ## Addition of Count Vectorizer\n",
    "#add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "\n",
    "## Not using these, since the values score isn't much better than with Count Vectorizer.\n",
    "#add_TruncatedSVD(pipe_spam_ham_features,grid_search_parameters)\n",
    "#add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "\n",
    "\n",
    "\n",
    "## Create a dictionary of all available ML Algos\n",
    "list_ml_algo['knn'] = 'knn'\n",
    "list_ml_algo['rf'] = 'randomforest'\n",
    "list_ml_algo['lr'] = 'logistic_regression'\n",
    "list_ml_algo['nb'] = 'multinomialNB'\n",
    "list_ml_algo['svc'] = 'svc_regression'\n",
    "list_ml_algo['gbc'] = 'gradient_boosting_classifer'\n",
    "list_ml_algo['dtc'] = 'decisiontree_classifer'\n",
    "\n",
    "\n",
    "## Kick off the pipeline Execution:\n",
    "\n",
    "## Iteration 1:\n",
    "## No Vectorizer\n",
    "\n",
    "count = 1\n",
    "\n",
    "while count < 3:\n",
    "    if count == 1:\n",
    "        add_count_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "    if count == 2:\n",
    "        add_tfidf_vectorizer(pipe_spam_ham_features,grid_search_parameters)\n",
    "        \n",
    "    for key, values in list_ml_algo.items():\n",
    "        ml_algo_name = 'add_' + values\n",
    "        returnValueIfAny = globals()[ml_algo_name](pipe_spam_ham_features,grid_search_parameters)\n",
    "\n",
    "        ## Setting up the pipeline\n",
    "        pipe_spam_ham = Pipeline(pipe_spam_ham_features)\n",
    "\n",
    "        classifier = str(pipe_spam_ham_features[-1:][0][1])\n",
    "\n",
    "        print(pipe_spam_ham)\n",
    "\n",
    "        print(grid_search_parameters)\n",
    "\n",
    "        ## Adding the GridSearch CV\n",
    "        gs_clf_pipe_spam_ham = GridSearchCV(pipe_spam_ham, grid_search_parameters, n_jobs=1, cv = cv_value, return_train_score=True)\n",
    "\n",
    "        ML_Pipeline_Processing_And_Metrics(run_id,X_train, y_train, X_test, y_test,grid_search_parameters, gs_clf_pipe_spam_ham, cv_value, classifier)\n",
    "\n",
    "        remove_vectorizer_ml_algo(key)\n",
    "\n",
    "    # remove_vectorizer_ml_algo('truncatedsvd')\n",
    "    remove_vectorizer_ml_algo('vect')\n",
    "    remove_vectorizer_ml_algo('tfidf')\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "## End of Program ..        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-25 22:14:04,570 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-07-25 22:14:04,572 : INFO : collecting all words and their counts\n",
      "2018-07-25 22:14:04,573 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2018-07-25 22:14:04,574 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-25 22:14:04,575 : INFO : collected 6 word types from a corpus of 9 raw words and 2 sentences\n",
      "2018-07-25 22:14:04,578 : INFO : Loading a fresh vocabulary\n",
      "2018-07-25 22:14:04,579 : INFO : min_count=2 retains 3 unique words (50% of original 6, drops 3)\n",
      "2018-07-25 22:14:04,580 : INFO : min_count=2 leaves 6 word corpus (66% of original 9, drops 3)\n",
      "2018-07-25 22:14:04,581 : INFO : deleting the raw counts dictionary of 6 items\n",
      "2018-07-25 22:14:04,581 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-07-25 22:14:04,582 : INFO : downsampling leaves estimated 0 word corpus (5.8% of prior 6)\n",
      "2018-07-25 22:14:04,583 : INFO : estimated required memory for 3 words and 150 dimensions: 5100 bytes\n",
      "2018-07-25 22:14:04,584 : INFO : resetting layer weights\n",
      "2018-07-25 22:14:04,585 : INFO : training model with 10 workers on 3 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-25 22:14:04,593 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,594 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,595 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,596 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,596 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,598 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,603 : INFO : EPOCH - 1 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,609 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,610 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,611 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,612 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,615 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,617 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,619 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,622 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,623 : INFO : EPOCH - 2 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,634 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,636 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,638 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,639 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,640 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,641 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,642 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,643 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,644 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,646 : INFO : EPOCH - 3 : training on 9 raw words (1 effective words) took 0.0s, 81 effective words/s\n",
      "2018-07-25 22:14:04,664 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,666 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,670 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,673 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,675 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,677 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,678 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,683 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,684 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,686 : INFO : EPOCH - 4 : training on 9 raw words (1 effective words) took 0.0s, 43 effective words/s\n",
      "2018-07-25 22:14:04,699 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,702 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,704 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,705 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,707 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,708 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,716 : INFO : EPOCH - 5 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,717 : INFO : training on a 45 raw words (2 effective words) took 0.1s, 15 effective words/s\n",
      "2018-07-25 22:14:04,719 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-07-25 22:14:04,720 : INFO : training model with 10 workers on 3 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-25 22:14:04,737 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,739 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,740 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,742 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,743 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,745 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,746 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,752 : INFO : EPOCH - 1 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,753 : WARNING : EPOCH - 1 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,771 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,773 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-25 22:14:04,775 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,776 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,778 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,780 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,781 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,783 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,788 : INFO : EPOCH - 2 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,789 : WARNING : EPOCH - 2 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,805 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,807 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,809 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,810 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,812 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,814 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,815 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,818 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,821 : INFO : EPOCH - 3 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,824 : WARNING : EPOCH - 3 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,837 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,839 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,840 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,842 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,850 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,851 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,853 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,854 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,856 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,857 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,859 : INFO : EPOCH - 4 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,860 : WARNING : EPOCH - 4 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,881 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,884 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,885 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,887 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,889 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,890 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,892 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,893 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,897 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,898 : INFO : EPOCH - 5 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,900 : WARNING : EPOCH - 5 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,918 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,921 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,922 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,924 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,926 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,927 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,929 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,936 : INFO : EPOCH - 6 : training on 9 raw words (1 effective words) took 0.0s, 55 effective words/s\n",
      "2018-07-25 22:14:04,937 : WARNING : EPOCH - 6 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,945 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,947 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,948 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,949 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,952 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,953 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,957 : INFO : EPOCH - 7 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:04,958 : WARNING : EPOCH - 7 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,970 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,971 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,972 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,973 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:04,975 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:04,976 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:04,978 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:04,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:04,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:04,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:04,982 : INFO : EPOCH - 8 : training on 9 raw words (1 effective words) took 0.0s, 78 effective words/s\n",
      "2018-07-25 22:14:04,983 : WARNING : EPOCH - 8 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:04,993 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:04,995 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:04,996 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:04,997 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:05,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-25 22:14:05,004 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:05,006 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:05,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:05,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:05,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:05,013 : INFO : EPOCH - 9 : training on 9 raw words (1 effective words) took 0.0s, 50 effective words/s\n",
      "2018-07-25 22:14:05,014 : WARNING : EPOCH - 9 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:05,030 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-07-25 22:14:05,033 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-07-25 22:14:05,035 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-07-25 22:14:05,036 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-07-25 22:14:05,038 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-07-25 22:14:05,039 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-07-25 22:14:05,041 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-25 22:14:05,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-25 22:14:05,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-25 22:14:05,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-25 22:14:05,047 : INFO : EPOCH - 10 : training on 9 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-25 22:14:05,048 : WARNING : EPOCH - 10 : supplied example count (2) did not equal expected count (5572)\n",
      "2018-07-25 22:14:05,050 : INFO : training on a 90 raw words (3 effective words) took 0.3s, 9 effective words/s\n",
      "2018-07-25 22:14:05,052 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 90)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports needed and logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "# build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "    raw_data,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)\n",
    "model.train(raw_data, total_examples=len(raw_data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
